{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d5579dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, GPT2LMHeadModel, pipeline\n",
    "\n",
    "class GPTLyric:\n",
    "    def __init__(self):\n",
    "        self.checkpoint = \"wvangils/GPT-Medium-Beatles-Lyrics-finetuned-newlyrics\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(self.checkpoint)\n",
    "        self.generator = pipeline('text-generation', model=self.model, tokenizer=self.tokenizer)\n",
    "    \n",
    "    def generate_lyric(self, text, max_length=128, sequences=1):\n",
    "        return self.generator(text, max_length=max_length, num_return_sequences=sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6614c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLyric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e50334c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"hello hows you doing?\\n\\nGood, hows it goin'?\\n\\nWell say hello hows you doing?\\n\\nGood, hows it goin'?\\n (Heavy rain)\\n\\n\\nLa la la la la la\\nLa la la la\\n\\n\\nOh la la la la la, la la la\\nLa la la la la, la la, la, la, la, la, la, la, la, la, la, la, la, la, la, la, la, la, la, la, la, la, la, la, la, la, la, la,\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_lyric(\"hello hows you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfbdcc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, pipeline\n",
    "import warnings\n",
    "\n",
    "class LyricsGenerator:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.pipeline = None\n",
    "    \n",
    "    def __download_model__(self, checkpoint=None):\n",
    "        if checkpoint == None:\n",
    "            checkpoint = \"wvangils/GPT-Medium-Beatles-Lyrics-finetuned-newlyrics\"\n",
    "            warnings.warn(\"No model_path passed using default model: GPT-Medium\")\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(checkpoint)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    \n",
    "    def __save_model__(self, model_path=None, tokenizer_path=None):\n",
    "        if model_path == None or tokenizer_path == None:\n",
    "            warnings.warn(\"No path passed using default path!\")\n",
    "            model_path = \"save_model\"\n",
    "            tokenizer_path = \"save_tokenizer\"\n",
    "        self.model.save_pretrained(model_path)\n",
    "        self.tokenizer.save_pretrained(tokenizer_path)\n",
    "    \n",
    "    def configure_pipeline(self, model_path=\"save_model\", model_tokenizer=\"save_tokenizer\"):\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_tokenizer)\n",
    "        self.pipeline = pipeline(\"text-generation\", model=self.model, tokenizer=self.tokenizer)\n",
    "\n",
    "    def generate_text(self, text, max_length=128):\n",
    "        if self.pipeline == None:\n",
    "            raise Exception(\"Pipeline not configured!\\nCall configure_pipeline\")\n",
    "        generated_text = self.pipeline(text, max_length=max_length)\n",
    "        return generated_text[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f79f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LyricsGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce735a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.configure_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c65330c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "text = model.generate_text(\"hello bunny how you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb2064c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello bunny how you doing?\n",
      "\n",
      "Mmm hmm good baby get back\n",
      "Good girl how you doing?\n",
      "Ah, how you doing?\n",
      "Mmm hmm good baby get back\n",
      "Good girl how you doing?\n",
      "Ah, how you doing?\n",
      "Mmm hmm good baby get back\n",
      "Good girl how you doing?\n",
      "Ah, how you doing?\n",
      "Mmm hmm good baby get back\n",
      "Good girl how you doing?\n",
      "Ah, how you doing?\n",
      "Mmm hmm good baby get back\n",
      "Good girl how you doing?\n",
      "Ah, how you doing?\n",
      "Mmm hmm good\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ec250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
